{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l-tifqSBVEE",
        "outputId": "e806f0f2-6747-4e75-b3c5-cc043791a03e"
      },
      "source": [
        "import numpy as np\n",
        "from flask import Flask, request, make_response\n",
        "import json\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "import re\n",
        "import os.path\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "import pandas as pd\n",
        "from tensorflow.keras import regularizers\n",
        "import numpy as np\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Flatten\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYjkmmf9Bc3J"
      },
      "source": [
        "data = open('/content/gdrive/MyDrive/models/nizami_books.txt',encoding='utf-8').read().lower().split('\\n')\n",
        "data=list(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lok3u1HKpstc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2cfmx3nBfk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a321da5-a451-478f-df7c-578beb637f9d"
      },
      "source": [
        "import keras.utils as ku\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "\n",
        "# Lowercase all text\n",
        "\n",
        "\n",
        "# Create Tokenizer object to convert words to sequences of integers\n",
        "tokenizer = Tokenizer(num_words = None, filters = '#$%&(),*+-<=>@[\\\\]^_`{|}~\\t\\n', lower = False)\n",
        "\n",
        "\n",
        "# Train tokenizer to the texts\n",
        "tokenizer.fit_on_texts(data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert list of strings into flat dataset of sequences of tokens\n",
        "sequences = []\n",
        "for line in data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences to ensure equal lengths\n",
        "max_seq_len = max([len(x) for x in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen = max_seq_len, padding = 'pre'))\n",
        "\n",
        "# Create n-grams sequence predictors and labels\n",
        "\n",
        "\n",
        "predictors, label = sequences[:, :-1], sequences[:, -1]\n",
        "label = ku.to_categorical(label, num_classes = total_words)\n",
        "input_len = max_seq_len - 1\n",
        "################\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 128, input_length = input_len))\n",
        "model.add(LSTM(150, return_sequences=True)),\n",
        "model.add(LSTM(100)),\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(total_words/2,activation='relu')),\n",
        "model.add(Dense(total_words, activation = 'softmax')),\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "if os.path.isfile('/content/gdrive/MyDrive/colabv2/trynizamivol4.h5') is False:\n",
        "    model.save('/content/gdrive/MyDrive/colabv2/trynizamivol4.h5')\n",
        "###############################################################################################\n",
        "import h5py\n",
        "\n",
        "\n",
        "\n",
        "#model=load_model('/content/gdrive/MyDrive/colabv2/trynizamivol3.h5')\n",
        "model.summary()\n",
        "################# 0.6 categorical_Crossentropy loss with 100 epochs  ##########################\n",
        "\n",
        "#\n",
        "model.fit(predictors, label, epochs = 50, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 9, 128)            3016832   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 9, 150)            167400    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               100400    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 11784)             1190184   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 23569)             277760665 \n",
            "=================================================================\n",
            "Total params: 282,235,481\n",
            "Trainable params: 282,235,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "2060/2060 [==============================] - 232s 96ms/step - loss: 9.1692\n",
            "Epoch 2/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 8.6641\n",
            "Epoch 3/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 8.2015\n",
            "Epoch 4/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 7.5203\n",
            "Epoch 5/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 6.6227\n",
            "Epoch 6/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 5.4719\n",
            "Epoch 7/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 4.3349\n",
            "Epoch 8/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 3.2400\n",
            "Epoch 9/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 2.4373\n",
            "Epoch 10/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 1.8883\n",
            "Epoch 11/50\n",
            "2060/2060 [==============================] - 195s 94ms/step - loss: 1.5304\n",
            "Epoch 12/50\n",
            "2060/2060 [==============================] - 195s 94ms/step - loss: 1.3079\n",
            "Epoch 13/50\n",
            "2060/2060 [==============================] - 194s 94ms/step - loss: 1.1576\n",
            "Epoch 14/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 1.0576\n",
            "Epoch 15/50\n",
            "2060/2060 [==============================] - 194s 94ms/step - loss: 0.9656\n",
            "Epoch 16/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.9154\n",
            "Epoch 17/50\n",
            "2060/2060 [==============================] - 193s 94ms/step - loss: 0.8623\n",
            "Epoch 18/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.8398\n",
            "Epoch 19/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.7992\n",
            "Epoch 20/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 0.7723\n",
            "Epoch 21/50\n",
            "2060/2060 [==============================] - 197s 95ms/step - loss: 0.7523\n",
            "Epoch 22/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.7397\n",
            "Epoch 23/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.7193\n",
            "Epoch 24/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.7091\n",
            "Epoch 25/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.6876\n",
            "Epoch 26/50\n",
            "2060/2060 [==============================] - 199s 97ms/step - loss: 0.6994\n",
            "Epoch 27/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 0.6743\n",
            "Epoch 28/50\n",
            "2060/2060 [==============================] - 194s 94ms/step - loss: 0.6664\n",
            "Epoch 29/50\n",
            "2060/2060 [==============================] - 194s 94ms/step - loss: 0.6676\n",
            "Epoch 30/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 0.6610\n",
            "Epoch 31/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 0.6474\n",
            "Epoch 32/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 0.6357\n",
            "Epoch 33/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 0.6343\n",
            "Epoch 34/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.6339\n",
            "Epoch 35/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.6255\n",
            "Epoch 36/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 0.6297\n",
            "Epoch 37/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 0.6215\n",
            "Epoch 38/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 0.6197\n",
            "Epoch 39/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.6128\n",
            "Epoch 40/50\n",
            "2060/2060 [==============================] - 197s 96ms/step - loss: 0.6001\n",
            "Epoch 41/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 0.6016\n",
            "Epoch 42/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.6079\n",
            "Epoch 43/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 0.5885\n",
            "Epoch 44/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.5909\n",
            "Epoch 45/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 0.5981\n",
            "Epoch 46/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.5907\n",
            "Epoch 47/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.5817\n",
            "Epoch 48/50\n",
            "2060/2060 [==============================] - 195s 95ms/step - loss: 0.5885\n",
            "Epoch 49/50\n",
            "2060/2060 [==============================] - 196s 95ms/step - loss: 0.5870\n",
            "Epoch 50/50\n",
            "2060/2060 [==============================] - 198s 96ms/step - loss: 0.5840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd31a3b51d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTJd1QhSkNJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e36f62-4692-4955-8a97-b4de9315faf9"
      },
      "source": [
        "next_words = 103\n",
        "import os\n",
        "import re\n",
        "seed_text=input()\n",
        "def poem(seed_text):  \n",
        "    for _ in range(1,next_words+1):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0)\n",
        "        output_word = ''\n",
        "        if _%4==0:\n",
        "            seed_text=seed_text + ','\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word=word\n",
        "                break\n",
        "        #if _%4==0:\n",
        "         #   output_word+='\\n'\n",
        "        seed_text += ' ' +output_word\n",
        "   \n",
        "    split_regex = re.compile(r'[,]')\n",
        "    sentences = [t.strip() for t in split_regex.split(seed_text)]\n",
        "    poem=[]\n",
        "    for s in sentences:\n",
        "        poem.append(s)\n",
        "    return poem"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgQhRclfk5GH",
        "outputId": "e1799f78-0209-471c-ffaf-03f4bda80657"
      },
      "source": [
        "poem('günəş')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['günəş kimi ucalıb yaraşıq',\n",
              " 'ol aləmə. əlimlə sənindir',\n",
              " 'bu dövran. əzəldən naşükürsən.',\n",
              " 'azad çöhrəsi. olubdur? olubdur?',\n",
              " 'nəydi olum zillət çəkəsən.',\n",
              " 'cahanda. haçandır fələk. qəlbinə.',\n",
              " 'olsun. görməz səni ağı',\n",
              " 'sənə neyləyib? qazancın. həmişə',\n",
              " 'qeydinə cahanın indi ovçuya',\n",
              " 'həmişə yerə. üstünə əzəldən.',\n",
              " 'çalma var. sənin. qəlb',\n",
              " 'açsın yenə də inanma;',\n",
              " 'olar. ver mana ver',\n",
              " 'ona. dedi ki: \"ay',\n",
              " 'canım burax; yaşa sənin.',\n",
              " 'sənin. atanı keçir onu',\n",
              " 'bu cahana bağlıdır. dolaşa?',\n",
              " 'əzəldən bil qopar günəşin.',\n",
              " 'qoşad nədir? məslək yolunu.',\n",
              " 'həmən. dəmirdən olsa da',\n",
              " 'bütün yağılar. sənin. mən.',\n",
              " 'o? havayı da. dünyada',\n",
              " 'olsun heç incə dalardı.',\n",
              " 'tərəf. yavəri can yalnız',\n",
              " 'sayağı sığındı yer qoynuna',\n",
              " 'sən uçmağın çətindir çətin.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}